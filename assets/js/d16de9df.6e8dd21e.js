"use strict";(globalThis.webpackChunkphysical_ai_capstone_docs=globalThis.webpackChunkphysical_ai_capstone_docs||[]).push([[944],{364(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"ai-integration/overview","title":"AI-Robot Integration (P5)","description":"User Story 5 - AI-Robot Integration: Vision-Language-Action Systems","source":"@site/docs/ai-integration/overview.md","sourceDirName":"ai-integration","slug":"/ai-integration/overview","permalink":"/physical-ai-capstone/docs/ai-integration/overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Platform Comparison (P4)","permalink":"/physical-ai-capstone/docs/simulation-platforms/overview"},"next":{"title":"Evaluation Framework (P6)","permalink":"/physical-ai-capstone/docs/evaluation/overview"}}');var t=i(4848),r=i(8453);const s={sidebar_position:1},a="AI-Robot Integration (P5)",l={},c=[{value:"Purpose",id:"purpose",level:2},{value:"Subsections to be Created",id:"subsections-to-be-created",level:2},{value:"VSLAM Navigation (~250 words)",id:"vslam-navigation-250-words",level:3},{value:"Vision-Language-Action Models (~250 words)",id:"vision-language-action-models-250-words",level:3},{value:"Multi-Modal Perception (~200 words)",id:"multi-modal-perception-200-words",level:3},{value:"Acceptance Criteria",id:"acceptance-criteria",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"ai-robot-integration-p5",children:"AI-Robot Integration (P5)"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"User Story 5"})," - AI-Robot Integration: Vision-Language-Action Systems"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Priority"}),": P5"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Word Count Target"}),": ~700 words (600-800 acceptable)"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Peer-Reviewed Sources Required"}),": \u22652 sources"]}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"Analyze advanced AI integration patterns including VSLAM navigation, vision-language-action models, speech processing, and multi-modal perception for natural human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"subsections-to-be-created",children:"Subsections to be Created"}),"\n",(0,t.jsx)(n.h3,{id:"vslam-navigation-250-words",children:"VSLAM Navigation (~250 words)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual SLAM problem definition"}),"\n",(0,t.jsx)(n.li,{children:"ORB-SLAM3 algorithm overview"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS GPU-accelerated VSLAM"}),"\n",(0,t.jsx)(n.li,{children:"Nav2 stack integration"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-action-models-250-words",children:"Vision-Language-Action Models (~250 words)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"VLA motivation and architecture"}),"\n",(0,t.jsx)(n.li,{children:"RT-1, RT-2 model examples"}),"\n",(0,t.jsx)(n.li,{children:"Training: large-scale robot data and sim-to-real"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 integration as action server"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"multi-modal-perception-200-words",children:"Multi-Modal Perception (~200 words)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sensor fusion approaches"}),"\n",(0,t.jsx)(n.li,{children:"Camera + LiDAR + IMU integration"}),"\n",(0,t.jsx)(n.li,{children:"Kalman filtering and extensions"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 robot_localization package"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"acceptance-criteria",children:"Acceptance Criteria"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Claims about VSLAM, VLA, multi-modal models traceable to recent AI/robotics papers (2022-2025)\r\n\u2705 Terms defined: VSLAM, VLA, ORB-SLAM3, RT-1, RT-2, sensor fusion\r\n\u2705 Word count 600-800 words\r\n\u2705 \u22652 peer-reviewed sources cited"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Status"}),": Awaiting P1-P4 completion before drafting"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);